# batch_covid.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Crear sesi√≥n Spark
spark = SparkSession.builder.appName("BatchCovidAnalysis").getOrCreate()

# Cargar dataset
df = spark.read.csv("country_wise_latest.csv", header=True, inferSchema=True)

print("üìä Estructura del archivo:")
df.printSchema()

print("üîπ Primeras filas del dataset:")
df.show(10, truncate=False)

# Limpieza b√°sica: eliminar duplicados y nulos en columnas clave
df_clean = df.dropDuplicates(["Country/Region"]).dropna(subset=["Confirmed", "Deaths"])

print("‚úÖ Datos limpiados correctamente")

# An√°lisis 1: Top 10 pa√≠ses con m√°s casos confirmados
top_confirmed = df_clean.orderBy(col("Confirmed").desc()).select("Country/Region", "Confirmed")
print("üåç Top 10 pa√≠ses con m√°s casos confirmados:")
top_confirmed.show(10, truncate=False)

# An√°lisis 2: Mortalidad por regi√≥n OMS
region_deaths = df_clean.groupBy("WHO Region").sum("Deaths").orderBy(col("sum(Deaths)").desc())
print("‚ö∞Ô∏è Muertes totales por regi√≥n OMS:")
region_deaths.show(truncate=False)

# An√°lisis 3: Tasa de recuperaci√≥n promedio por regi√≥n
region_recovery = df_clean.groupBy("WHO Region").avg("Recovered / 100 Cases")
print("üíö Promedio de recuperaci√≥n (%):")
region_recovery.show(truncate=False)

# Guardar resultados procesados
top_confirmed.write.mode("overwrite").csv("resultados_covid/top_confirmed")
region_deaths.write.mode("overwrite").csv("resultados_covid/region_deaths")
region_recovery.write.mode("overwrite").csv("resultados_covid/region_recovery")

print("üíæ Resultados guardados en carpeta 'resultados_covid/'")
print("‚úÖ Procesamiento batch finalizado correctamente.")
